                                        A SMALL STUDY ON NETWORK'S DEPTH, GRADIENTS, AND LEARNING BEHAVIOR
                                        

I/Introduction
-Motivation:
This study was carried out mainly out of curiosity about how neural networks are designed, especially how the depth of a network relates to its behavior during learning. Instead of starting from the assumption that deeper models are always better, this work aims to observe how changing the number of layers affects the learning process under controlled conditions. In particular, the study explores whether there appears to be a minimum or maximum depth at which the model learns more effectively or more stably.
This study does not aim to propose an optimal architecture or achieve high performance. Rather, it is intended as a small observational reference that highlights differences in learning behavior as network depth increases. The results may be useful for future projects or studies that require a basic but careful understanding of how depth influences learning in neural networks.
-Paper philosophy (observational-first):
This study is conducted in a context where I did not begin with clear knowledge, but only a small set of initial hypotheses about how network depth might affect gradient norms, accuracy, and loss behavior. In short, the study is carried out in an “on-the-fly” manner, where observations are formed during experimentation rather than being fixed in advance. Throughout the process, I record model performance, loss behavior, and gradient norms, together with corresponding loss curves and gradient norm plots. At the end of each dataset, these observations are summarized to gradually improve my understanding of how depth influences learning behavior.
In this study, results are recorded as honestly as possible, without adjusting or beautifying outcomes. Both visual outputs and textual notes are preserved as direct representations of the experiments. This can be seen most clearly in the inclusion of raw loss samples for each observation, which are presented without selection or filtering. During the research process, several issues emerged, particularly in the first dataset (Circle), such as dead ReLU activations, the absence of certain metrics, and the need to re-establish a baseline due to unsuitable hyperparameter choices across the project. These issues are explicitly documented and carried forward as considerations when transitioning to the second dataset (Nested Rings).
Regarding the scope of this study, no external deep learning libraries or predefined modules are used to construct the neural networks. All network components, data generation procedures, and datasets are implemented with the support of NumPy, and visualized using Matplotlib. This deliberate choice is made to help me better understand neural network structures, identify issues arising from activation functions, and learn how to address them directly, rather than relying on or modifying existing deep learning library implementations.
II/Research Questions and Working Hypotheses
-Research questions: 
This study is motivated by a simple question I had when first learning about neural networks: does increasing depth actually make a model better at learning? Rather than interpreting “better” only in terms of accuracy, this study expands the question to consider multiple aspects of the learning process.
Specifically, the research focuses on how changes in network depth affect not only final accuracy, but also gradient norms, loss behavior, and the shape of loss curves during training. By shifting the focus away from accuracy alone, the study aims to compare learning behavior across different depths from several perspectives, instead of relying on a single performance metric.
-Working Hypotheses:
At the beginning of this study, I initially believed that increasing network depth would consistently lead to better learning on nonlinear datasets. This belief came from the idea that adding depth also increases the number of neurons involved in processing the data, which could allow the model to represent more complex patterns and learn more effectively.
However, as I reflected on this assumption before conducting the experiments, this initial belief was gradually replaced by a more cautious hypothesis. I began to consider that increasing depth might improve learning only up to a certain point. Beyond that point, making the network deeper could introduce unnecessary complexity, potentially leading to unstable training behavior and reduced effectiveness across metrics such as accuracy, loss behavior, and gradient norms. This study is therefore guided by the hypothesis that depth improves learning only within a limited range, rather than indefinitely.
III/Experimental Setup and Scope
-From-scratch NN : All modules used in this study were implemented entirely from scratch, with NumPy serving as the only supporting library for matrix operations. No deep learning frameworks such as PyTorch, TensorFlow, or Keras were used at any stage of the project.
Several core modules were written and reused across experiments, including activation functions (ReLU and Sigmoid), linear layers, the loss function (Binary Cross-Entropy), the training loop, and a gradient diagnostics module used to measure gradient norms during training.
-Controlled variables : As stated in the project README, multiple experimental factors were kept fixed across all runs. These included the data generation logic, training procedure, loss function, activation functions, weight initialization strategy, and random seeds.
By holding these variables constant and isolating network depth as the only changing factor, the study aims to make observation of loss behavior and gradient dynamics more direct and interpretable, in line with the exploratory nature of the research.
During the course of experimentation, several issues were encountered, including dead ReLU activations and unstable training caused by unsuitable weight initialization. In particular, these issues required rebaselining at depths 2 and 4 in the circle dataset. To preserve experimental integrity and consistency, the affected experiments were restarted. Adjustments such as learning rate changes and the adoption of He initialization are documented in the Fairness in Depth Comparison section.

-Dataset design (Circle → Nested Rings): The study evaluates networks of varying depth (1, 2, 4, and 8 layers) across two synthetic datasets with different levels of complexity: the circle dataset and the nested rings dataset.
The circle dataset serves as an initial sanity check, providing a simple nonlinear structure to verify the correctness of the implementation and observe basic loss behavior before moving to a more challenging setting. The nested rings dataset is then used as a stress test, where differences in loss curves, gradient norms, and training stability become more pronounced as depth increases.
-Scope: 
This study is limited to fully connected neural networks implemented from scratch, using fixed datasets and a fixed training procedure. The scope does not include convolutional architectures, large-scale real-world datasets, or comparisons with state-of-the-art models.
These constraints are intentionally chosen to prioritize clarity and interpretability over performance, allowing the effects of network depth on learning behavior to be observed more directly.


IV/Observed Failures ,The Absence Of Crucial Metrics And Training Instabilities
-Observed Failures :
    +Dead ReLU.
In Dataset 1 (circle), a dead ReLU phenomenon was observed following the baseline experiments. This issue arises when ReLU activations progressively truncate the variance of the pre-activation values, leading to gradients that are extremely small or effectively zero when propagated back to the initial layers. As a result, weight updates become largely irrelevant, preventing meaningful training progress.
This behavior was identified as a technical issue rather than a feature-related limitation. Leaving the issue unaddressed would distort the original purpose of the study by shifting the focus from analyzing learning metrics to merely evaluating model survivability. The problem was resolved by adopting He initialization, which was then consistently applied from the restarted experiments in Dataset 1 through all experiments in Dataset 2.
    +Initial hyperparameter configuration.
Within this study, the primary hyperparameter adjustment involved the learning rate. The learning rate was reduced by a factor of 100 (from 0.1 to 0.001) to preserve the integrity of the research objective. This adjustment allowed deeper models to remain trainable and stable, while not significantly affecting models that were already able to converge under the original setting.
Following this change, training behavior across all depths became noticeably more stable. The rebaselining of Dataset 1 included this learning rate modification, which was subsequently applied consistently to all models evaluated on both datasets.
    +Experimental limitations in Dataset 1.
Several important metrics were initially absent during the early stages of Dataset 1 experiments. These included gradient-related metrics (mean gradient norm, coefficient of variation, and gradient norm standard deviation), training and testing accuracy, as well as repeated runs across multiple random seeds (10 runs).
The absence of accuracy metrics and repeated trials reduced the objectivity of early observations and increased the risk of luck-based or cherry-picked interpretations. Gradient-related metrics were later introduced to provide a more comprehensive view of training behavior, beyond loss and accuracy alone.
These limitations were addressed toward the later stage of Dataset 1 and fully adopted in Dataset 2. The updated experimental protocol and reflections are documented in dataset1_reflection_dataset2_protocol.md.
-Training Instability : 
Dataset 1 (circle): Increasing network depth does not lead to a noticeable change in performance, as both training and testing accuracy remain relatively similar across models. The overall shapes of the loss curves are also largely comparable, especially during the early stages of training.
Interestingly, models with relatively larger depth within the context of this project tend to exhibit smoother loss behavior. Variations in the loss curves are not strongly pronounced when compared to models with moderate depth. As a result, training instability—defined here as difficulty in convergence or oscillatory behavior—does not clearly manifest in loss samples or loss curve visualizations. Only mild oscillations are observed during the middle and late stages of training.
However, gradient-based metrics reveal a different pattern. The mean gradient norm, coefficient of variation, and gradient norm standard deviation all show a consistent increasing trend as network depth increases. Both the numerical metrics and gradient norm plots indicate an overall growth in gradient magnitude and variability with depth, despite the relatively stable loss behavior.

Dataset 2 (nested rings) : In contrast to the circle dataset, the nested rings dataset reveals a clearer relationship between network depth and learning behavior. Testing accuracy increases as depth grows, but this improvement appears to saturate beyond a certain point. The most noticeable gains occur from the baseline model to depth 2 and depth 4, while performance plateaus at depth 6 and depth 8.
Loss curve shapes also differ more clearly across depths compared to Dataset 1. The initial slopes are steeper, indicating more active early learning. For the baseline model, a clear plateau phase is not observed; instead, continuous improvement is recorded throughout training, although the rate of improvement slows during the late stage. Notably, training instability is most evident in the baseline model rather than in deeper architectures.
An additional irregularity emerges when increasing depth from 2 to 4. The depth-4 model exhibits the largest degree of instability, as observed through loss curve fluctuations, regardless of comparison with the baseline model. At the same time, this model achieves the highest testing accuracy. Deeper models, such as depth 6 and depth 8, display comparatively smaller and more subtle instability than the depth-4 model.
Gradient-based metrics further clarify this behavior. The mean gradient norm and coefficient of variation increase sharply from the baseline model up to depth 6, while depth 8 produces values close to depth 6, suggesting a possible saturation effect. In contrast, the gradient norm standard deviation follows a pattern similar to testing accuracy: it peaks at depth 4 and then decreases or stabilizes in deeper models.
V/Results Overview
VI/Analysis / Interpretation
VII/Limitations
IX/Conclusion