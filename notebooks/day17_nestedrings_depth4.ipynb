{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbae6c-56d9-41fb-9724-da79adc00e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from module.dynamic_architect import Neural_Network\n",
    "from module.layers import Linear\n",
    "from module.activations import ReLU, Sigmoid\n",
    "from module.losses import BCE\n",
    "from module.train_loop import train\n",
    "from data.generate import generate_nestedrings_data \n",
    "from module.gradient_diagnostics import gradient_norm, training_norm\n",
    "\n",
    "def Predict(X,model): \n",
    "    A = X \n",
    "    for layer in model: \n",
    "        A = layer.forward(A) \n",
    "    return A\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "\n",
    "\n",
    "for seed in range(10):\n",
    "    \n",
    "    np.set_printoptions(suppress=True) \n",
    "    depth4 = Neural_Network([2,16,16,16,16,1]) \n",
    "    model = depth4.layers \n",
    "    X,y =  generate_nestedrings_data(2000,seed) \n",
    "    loss_func = BCE()\n",
    "    loss_history = train(model,loss_func,X,y,1000,32,0.001) \n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    y_hat = Predict(X,model) \n",
    "    y_pred = (y_hat > 0.5).astype(int)\n",
    "    training_accuracy = np.mean(y_pred == y)\n",
    "    \n",
    "    \n",
    "    X_test,y_test =  generate_nestedrings_data(400,seed+100)\n",
    "    y_hat_test = Predict(X_test,model)\n",
    "    y_pred_test = (y_hat_test > 0.5).astype(int)\n",
    "    testing_accuracy = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    train_accs.append(training_accuracy)\n",
    "    test_accs.append(testing_accuracy)\n",
    "\n",
    "print(f\"TRAIN acc: {np.mean(train_accs):.4f} ± {np.std(train_accs):.4f}\")\n",
    "print(f\"TEST  acc: {np.mean(test_accs):.4f} ± {np.std(test_accs):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299923b3-8376-4a67-a364-ec1fd28994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_per_epoch = int(np.ceil(len(X) / 32))\n",
    "epoch_loss = [\n",
    "    np.mean(loss_history[i*batch_per_epoch:(i+1)*batch_per_epoch])\n",
    "    for i in range(1000)\n",
    "]\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(epoch_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Training Loss\") \n",
    "plt.title(\"Training loss Curve(depth4)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e46f6-f60f-431a-997c-d88012538de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_100 = epoch_loss[900:]\n",
    "for i in range(0,100,10): \n",
    "    print(\" \".join(f\"{x:8.4f}\" for x in last_100[i:i+10]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0234947-d6f3-4aa5-bab7-bde3af5ee3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_norm = training_norm(model,loss_func,X,y,1000,32,0.001) \n",
    "print(\"AVG GRADIENT NORM: \", np.mean(epochs_norm))\n",
    "print(\"GRADIENT NORM STANDARD DEVIATION: \",np.std(epochs_norm))\n",
    "print(\"COEFFICIENT OF VARIATION: \", np.std(epochs_norm)/np.mean(epochs_norm))\n",
    "\n",
    "plt.plot(epochs_norm)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Gradient Norm\") \n",
    "plt.title(\"Gradient Norm Trend\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
