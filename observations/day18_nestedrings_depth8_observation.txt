Day 18 - Depth8 Nestedrings Classification 
Loss observation: 
(C1) Global Trend : 
-Exhibits an overall decreasing trend. 
(C2) Stability : 
-Despite having the most layers among the models on which the experiments were rerun, the depth-8 model is notably stable with mild spikes spread evenly across all stages
(C3) Plateau Phase: 
- A plateau-like phase is observed after approximately 100 epochs
(C4) Late-stage gains: 
-Diminishing late-stage improvement is noticed, consistent with the behavior seen in shallower models.




RAW LOSS SAMPLES (LAST 100 EPOCHS): loss samples vary within a similar bounded range compared to shallower models
(THESE SAMPLES ARE REPRESENTATIVE)
0.0420   0.0324   0.0331   0.0437   0.0399   0.0329   0.0401   0.0455   0.0358   0.0271
  0.0283   0.0478   0.0270   0.0361   0.0300   0.0373   0.0358   0.0331   0.0261   0.0347
  0.0389   0.0435   0.0281   0.0268   0.0264   0.0331   0.0360   0.0361   0.0535   0.0281
  0.0359   0.0341   0.0455   0.0355   0.0330   0.0259   0.0330   0.0523   0.0304   0.0340
  0.0293   0.0305   0.0321   0.0253   0.0480   0.0287   0.0308   0.0337   0.0393   0.0375
  0.0277   0.0406   0.0279   0.0205   0.0374   0.0278   0.0386   0.0372   0.0318   0.0311
  0.0368   0.0251   0.0346   0.0444   0.0420   0.0323   0.0333   0.0399   0.0439   0.0321
  0.0334   0.0249   0.0361   0.0253   0.0351   0.0341   0.0268   0.0414   0.0335   0.0333
  0.0419   0.0348   0.0387   0.0320   0.0262   0.0335   0.0389   0.0521   0.0263   0.0377
  0.0262   0.0269   0.0291   0.0360   0.0399   0.0331   0.0289   0.0405   0.0334   0.0385
(C5) Minimal Interpretation : 
-The loss shows no considerable features beyond approximately 100 epochs. No late-stage gains are observed. Under the fixed learning rate and training regime, increasing depth beyond six layers does not appear to yield further optimization or generalization benefits on the nested rings task.

WRITTEN OBSERVATION:
-The model displays a common decreasing trend and maintains high stability with mild spikes. After about 100 epochs, the model enters a plateau-like phase with limited late-stage gains. Compared to depth-6, this behavior indicates diminishing returns rather than optimization difficulty.
- Despite being the most complex model, it does not struggle to optimize under the fixed training condition (lr = 0.001 and 1000 epochs) and shows similar behavior to other models (depth-2, depth-4, depth-6) except for the baseline configuration.
TRAIN ACCURACY (MEAN OF 10 RUNS): 0.9849 ± 0.0049
TEST  ACCURACY (MEAN OF 10 RUNS): 0.9732 ± 0.0043


Gradient Norm Observation : 
AVG GRADIENT NORM:  27.884691350961372
GRADIENT NORM STANDARD DEVIATION:  4.429147543650532
COEFFICIENT OF VARIATION:  0.15883796194494473
NOTABLE GRADIENT NORM BEHAVIOR (GRADIENT REGIME) : The gradient norms fluctuate within a controlled range (~13 to ~47) with no strong monotonic trend, shows a gradient pattern similar to the depth-6 model. The relatively low coefficient of variation suggests stable gradient propagation. There is no indication of severe vanishing or exploding gradients.